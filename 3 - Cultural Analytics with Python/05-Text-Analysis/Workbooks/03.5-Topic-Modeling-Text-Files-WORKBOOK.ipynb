{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling — Workbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these lessons, we're learning about a text analysis method called *topic modeling*. This method will help us identify the main topics or discourses within a collection of texts.\n",
    "\n",
    "Note: If you don't have the Java Development Kit and Mallet installed, you can run this [notebook in the cloud here](https://mybinder.org/v2/gh/INFO1350/Intro-CA-SP21/master?urlpath=lab/tree/book/05-Text-Analysis/03.5-Topic-Modeling-Text-Files-WORKBOOK.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this particular lesson, we're going to use [Little MALLET Wrapper](https://github.com/maria-antoniak/little-mallet-wrapper), a Python wrapper for [MALLET](http://mallet.cs.umass.edu/topics.php), to topic model 379 obituaries published by *The New York Times*. This dataset is based on data originally collected by Matt Lavin for his *Programming Historian* [TF-IDF tutorial](https://programminghistorian.org/en/lessons/analyzing-documents-with-tfidf#lesson-dataset). I have re-scraped the obituaries so that the subject's name and death year is included in each text file name, and I have added 13 more [\"Overlooked\"](https://www.nytimes.com/interactive/2018/obituaries/overlooked.html) obituaries, including [Karen Spärck Jones](https://www.nytimes.com/2019/01/02/obituaries/karen-sparck-jones-overlooked.html), the computer scientist who introduced TF-IDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you have the Java Development Kit installed and configured properly? Test it out by running this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: javac <options> <source files>\n",
      "where possible options include:\n",
      "  -g                         Generate all debugging info\n",
      "  -g:none                    Generate no debugging info\n",
      "  -g:{lines,vars,source}     Generate only some debugging info\n",
      "  -nowarn                    Generate no warnings\n",
      "  -verbose                   Output messages about what the compiler is doing\n",
      "  -deprecation               Output source locations where deprecated APIs are used\n",
      "  -classpath <path>          Specify where to find user class files and annotation processors\n",
      "  -cp <path>                 Specify where to find user class files and annotation processors\n",
      "  -sourcepath <path>         Specify where to find input source files\n",
      "  -bootclasspath <path>      Override location of bootstrap class files\n",
      "  -extdirs <dirs>            Override location of installed extensions\n",
      "  -endorseddirs <dirs>       Override location of endorsed standards path\n",
      "  -proc:{none,only}          Control whether annotation processing and/or compilation is done.\n",
      "  -processor <class1>[,<class2>,<class3>...] Names of the annotation processors to run; bypasses default discovery process\n",
      "  -processorpath <path>      Specify where to find annotation processors\n",
      "  -parameters                Generate metadata for reflection on method parameters\n",
      "  -d <directory>             Specify where to place generated class files\n",
      "  -s <directory>             Specify where to place generated source files\n",
      "  -h <directory>             Specify where to place generated native header files\n",
      "  -implicit:{none,class}     Specify whether or not to generate class files for implicitly referenced files\n",
      "  -encoding <encoding>       Specify character encoding used by source files\n",
      "  -source <release>          Provide source compatibility with specified release\n",
      "  -target <release>          Generate class files for specific VM version\n",
      "  -profile <profile>         Check that API used is available in the specified profile\n",
      "  -version                   Version information\n",
      "  -help                      Print a synopsis of standard options\n",
      "  -Akey[=value]              Options to pass to annotation processors\n",
      "  -X                         Print a synopsis of nonstandard options\n",
      "  -J<flag>                   Pass <flag> directly to the runtime system\n",
      "  -Werror                    Terminate compilation if warnings occur\n",
      "  @<filename>                Read options and filenames from file\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!javac "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If you get the message, `Usage: javac <options> <source files>...`, then JDK is installed and set up!  \n",
    "- If you get some version of the message, `javac: command not found`, then it is not installed or configured properly, and we recommend running this notebook in the cloud for today."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install `little_mallet_wrapper` and the data viz library `seaborn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install little_mallet_wrapper\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To install the most updated version of little_mallet_wrapper:\n",
    "#!!pip install git+https://github.com/maria-antoniak/little-mallet-wrapper.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's `import` the `little_mallet_wrapper` and the data viz library `seaborn`. We're also going to import [`glob`](https://docs.python.org/3/library/glob.html) and [`pathlib`](https://docs.python.org/3/library/pathlib.html#basic-use) for working with files and the file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import little_mallet_wrapper\n",
    "import seaborn\n",
    "import glob\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Training Data From Text Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we topic model the *NYT* obituaries, we need to process the text files and prepare them for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the necessary text files, we're going to make a variable and assign it the file path for the directory that contains the text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"../texts/history/NYT-Obituaries/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we're going to use the `glob.gob()` function to make a list of all (`*`) the `.txt` files in that directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(f\"{directory}/*.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\"\"A century before the dawn of the computer age, Ada Lovelace imagined\n",
    "                the modern-day, general-purpose computer. It could be programmed to \n",
    "                follow instructions, she wrote in 1843.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "little_mallet_wrapper.process_string(sample_text, numbers='remove')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we process our texts with the function `little_mallet_wrapper.process_string()`. This function will take every individual text file, transform all the text to lowercase as well as remove stopwords, punctuation, and numbers, and then add the processed text to our master list `training_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a list of all pre-processed NYT obituaries\n",
    "training_data = []\n",
    "\n",
    "for file in files:\n",
    "    text = open(file, encoding='utf-8').read()\n",
    "    processed_text = little_mallet_wrapper.process_string(text, numbers='remove')\n",
    "    training_data.append(processed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're also making a master list of the original text of the obituaries for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a list of all original NYT obituaries (not pre-processed)\n",
    "original_texts = []\n",
    "\n",
    "for file in files:\n",
    "    text = open(file, encoding='utf-8').read()\n",
    "    original_texts.append(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we extract the relevant part of each file name by using [`Path().stem`](https://docs.python.org/3/library/pathlib.html#pathlib.PurePath.stem), which conveniently extracts just the last part of the file path without the \".txt\" file extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "obit_titles = [Path(file).stem for file in files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Training Data Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get training data summary statistics by using the function ```little_mallet_wrapper.print_dataset_stats()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "little_mallet_wrapper.print_dataset_stats(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Topic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set MALLET Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Little MALLET Wrapper is a Python package built around MALLET, we first need to tell it where the bigger, Java-based MALLET lives.\n",
    "\n",
    "We're going to make a variable called `path_to_mallet` and assign it the file path of our MALLET program. We need to point it, specifically, to the \"mallet\" file inside the \"bin\" folder inside the \"mallet-2.0.8\" folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_mallet = 'mallet-2.0.8/bin/mallet'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test out if you filepath to Mallet is correct, you can put the filepath in the code below and see if it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A tool for creating instance lists of feature vectors from comma-separated-values\n",
      "--help TRUE|FALSE\n",
      "  Print this command line option usage information.  Give argument of TRUE for longer documentation\n",
      "  Default is false\n",
      "--prefix-code 'JAVA CODE'\n",
      "  Java code you want run before any other interpreted code.  Note that the text is interpreted without modification, so unlike some other Java code options, you need to include any necessary 'new's when creating objects.\n",
      "  Default is null\n",
      "--config FILE\n",
      "  Read command option values from a file\n",
      "  Default is null\n",
      "--input FILE\n",
      "  The file containing data to be classified, one instance per line\n",
      "  Default is null\n",
      "--output FILE\n",
      "  Write the instance list to this file; Using - indicates stdout.\n",
      "  Default is text.vectors\n",
      "--line-regex REGEX\n",
      "  Regular expression containing regex-groups for label, name and data.\n",
      "  Default is ^(\\S*)[\\s,]*(\\S*)[\\s,]*(.*)$\n",
      "--label INTEGER\n",
      "  The index of the group containing the label string.\n",
      "   Use 0 to indicate that the label field is not used.\n",
      "  Default is 2\n",
      "--name INTEGER\n",
      "  The index of the group containing the instance name.\n",
      "   Use 0 to indicate that the name field is not used.\n",
      "  Default is 1\n",
      "--data INTEGER\n",
      "  The index of the group containing the data.\n",
      "  Default is 3\n",
      "--use-pipe-from FILE\n",
      "  Use the pipe and alphabets from a previously created vectors file.\n",
      "   Allows the creation, for example, of a test set of vectors that are\n",
      "   compatible with a previously created set of training vectors\n",
      "  Default is text.vectors\n",
      "--keep-sequence [TRUE|FALSE]\n",
      "  If true, final data will be a FeatureSequence rather than a FeatureVector.\n",
      "  Default is false\n",
      "--keep-sequence-bigrams [TRUE|FALSE]\n",
      "  If true, final data will be a FeatureSequenceWithBigrams rather than a FeatureVector.\n",
      "  Default is false\n",
      "--label-as-features [TRUE|FALSE]\n",
      "  If true, parse the 'label' field as space-delimited features.\n",
      "     Use feature=[number] to specify values for non-binary features.\n",
      "  Default is false\n",
      "--remove-stopwords [TRUE|FALSE]\n",
      "  If true, remove a default list of common English \"stop words\" from the text.\n",
      "  Default is false\n",
      "--replacement-files FILE [FILE ...]\n",
      "  files containing string replacements, one per line:\n",
      "    'A B [tab] C' replaces A B with C,\n",
      "    'A B' replaces A B with A_B\n",
      "  Default is (null)\n",
      "--deletion-files FILE [FILE ...]\n",
      "  files containing strings to delete after replacements but before tokenization (ie multiword stop terms)\n",
      "  Default is (null)\n",
      "--stoplist-file FILE\n",
      "  Instead of the default list, read stop words from a file, one per line. Implies --remove-stopwords\n",
      "  Default is null\n",
      "--extra-stopwords FILE\n",
      "  Read whitespace-separated words from this file, and add them to either \n",
      "   the default English stoplist or the list specified by --stoplist-file.\n",
      "  Default is null\n",
      "--stop-pattern-file FILE\n",
      "  Read regular expressions from a file, one per line. Tokens matching these regexps will be removed.\n",
      "  Default is null\n",
      "--preserve-case [TRUE|FALSE]\n",
      "  If true, do not force all strings to lowercase.\n",
      "  Default is false\n",
      "--encoding STRING\n",
      "  Character encoding for input file\n",
      "  Default is UTF-8\n",
      "--token-regex REGEX\n",
      "  Regular expression used for tokenization.\n",
      "   Example: \"[\\p{L}\\p{N}_]+|[\\p{P}]+\" (unicode letters, numbers and underscore OR all punctuation) \n",
      "  Default is \\p{L}[\\p{L}\\p{P}]+\\p{L}\n",
      "--print-output [TRUE|FALSE]\n",
      "  If true, print a representation of the processed data\n",
      "   to standard output. This option is intended for debugging.\n",
      "  Default is false\n"
     ]
    }
   ],
   "source": [
    "!mallet-2.0.8/bin/mallet import-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: Incorrect-Folder/mallet-2.0.8/bin/mallet: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!Incorrect-Folder/mallet-2.0.8/bin/mallet import-file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If MALLET is located in another directory or not set up properly, you will get a message \"No such file or directory.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Number of Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to make a variable `num_topics` and assign it the number of topics we want returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Topic Model Output Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to tell Little MALLET Wrapper where to find and output all of our topic modeling results. The code below will set Little MALLET Wrapper up to output your results inside a directory called \"topic-model-output\" and a subdirectory called \"NYT-Obits\", all of which will be inside your current directory.\n",
    "\n",
    "If you'd like to change this output location, simply change `output_directory_path` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training data (our pre-processed text files)\n",
    "training_data = training_data\n",
    "\n",
    "#Change to your desired output directory\n",
    "output_directory_path = 'topic-model-output/NYT-Obits-Workbook'\n",
    "\n",
    "#No need to change anything below here\n",
    "Path(f\"{output_directory_path}\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "path_to_training_data           = f\"{output_directory_path}/training.txt\"\n",
    "path_to_formatted_training_data = f\"{output_directory_path}/mallet.training\"\n",
    "path_to_model                   = f\"{output_directory_path}/mallet.model.{str(num_topics)}\"\n",
    "path_to_topic_keys              = f\"{output_directory_path}/mallet.topic_keys.{str(num_topics)}\"\n",
    "path_to_topic_distributions     = f\"{output_directory_path}/mallet.topic_distributions.{str(num_topics)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Topic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we import our training data with `little_mallet_wrapper.quick_train_topic_model()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "little_mallet_wrapper.quick_train_topic_model(path_to_mallet,\n",
    "                                              output_directory_path,\n",
    "                                              num_topics,\n",
    "                                              training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Topics and Top Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To examine the 15 topics that the topic model extracted from the *NYT* obituaries, run the cell below. This code uses the `little_mallet_wrapper.load_topic_keys()` function to read and process the MALLET topic model output from your computer, specifically the file \"mallet.topic_keys.15\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = little_mallet_wrapper.load_topic_keys(path_to_topic_keys)\n",
    "\n",
    "for topic_number, topic in enumerate(topics):\n",
    "    print(f\"✨Topic {topic_number}✨\\n\\n{topic}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Topic Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MALLET also calculates the likely mixture of these topics for every single obituary in the corpus. This mixture is called a probability distribution.\n",
    "\n",
    "To get the topic distributions, we're going to use the `little_mallet_wrapper.load_topic_distributions()` function, which will read and process the MALLET topic model output, specifically the file \"mallet.topic_distributions.15\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_distributions = little_mallet_wrapper.load_topic_distributions(path_to_topic_distributions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the 32nd topic distribution in this list of `topic_distributions`, which corresponds to Marilyn Monroe's obituary, we will see a list of 15 probabilities. This  list corresponds to the likelihood that each of the 15 topics exists in Marilyn Monroe's obituary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [],
   "source": [
    "topic_distributions[32]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a bit easier to understand if we pair these probabilities with the topics themselves. As you can see below, Topic 0 \"miss film theater movie broadway films\" has a relatively high probability of existing in Marilyn Monroe's obituary `.202` while Topic 5 \"soviet hitler german germany stalin union\" has a relatively low probability `.002`. This seems to comport with what we know about Marilyn Monroe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [],
   "source": [
    "obituary_to_check = \"1962-Marilyn-Monroe\"\n",
    "\n",
    "obit_number = obit_titles.index(obituary_to_check)\n",
    "\n",
    "print(f\"Topic Distributions for {obit_titles[obit_number]}\\n\")\n",
    "for topic_number, (topic, topic_distribution) in enumerate(zip(topics, topic_distributions[obit_number])):\n",
    "    print(f\"✨Topic {topic_number} {topic[:6]} ✨\\nProbability: {round(topic_distribution, 3)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Heatmap of Topics and Texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize and compare these topic probability distributions with a heatmap by using the `little_mallet_wrapper.plot_categories_by_topics_heatmap()` function.\n",
    "\n",
    "We have everything we need for the heatmap except for our list of target_labels, the sample of texts that we’d like to visualize and compare with the heatmap. Below we make our list of desired target labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_labels = ['1852-Ada-Lovelace', '1885-Ulysses-Grant',\n",
    "                 '1900-Nietzsche', '1931-Ida-B-Wells', '1940-Marcus-Garvey',\n",
    "                 '1941-Virginia-Woolf', '1954-Frida-Kahlo', '1962-Marilyn-Monroe',\n",
    "                 '1963-John-F-Kennedy', '1964-Nella-Larsen', '1972-Jackie-Robinson',\n",
    "                 '1973-Pablo-Picasso', '1984-Ray-A-Kroc','1986-Jorge-Luis-Borges', '1991-Miles-Davis',\n",
    "                 '1992-Marsha-P-Johnson', '1993-Cesar-Chavez']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'd like to make a random list of target labels, you can uncomment and run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import random\n",
    "#target_labels = random.sample(obit_titles, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "little_mallet_wrapper.plot_categories_by_topics_heatmap(obit_titles,\n",
    "                                      topic_distributions,\n",
    "                                      topics, \n",
    "                                      output_directory_path + '/categories_by_topics.pdf',\n",
    "                                      target_labels=target_labels,\n",
    "                                      dim= (13, 9)\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Top Titles Per Topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also display the obituaries that have the highest probability for every topic with the `little_mallet_wrapper.get_top_docs()` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we `zip` together the training data (pre-processed docs) and list of titles, as well as the training data and list of original texts. Then we turn them into dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_obit_titles = dict(zip(training_data, obit_titles))\n",
    "training_data_original_text = dict(zip(training_data, original_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we'll make our own function `display_top_titles_per_topic()` that will display the top text titles for every topic. This function accepts a given `topic_number` as well as a desired `number_of_documents` to display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_top_titles_per_topic(topic_number=0, number_of_documents=5):\n",
    "    \n",
    "    print(f\"✨Topic {topic_number}✨\\n\\n{topics[topic_number]}\\n\")\n",
    "\n",
    "    for probability, document in little_mallet_wrapper.get_top_docs(training_data, topic_distributions, topic_number, n=number_of_documents):\n",
    "        print(round(probability, 4), training_data_obit_titles[document] + \"\\n\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Topic 0**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To display the top 5 obituary titles with the highest probability of containing Topic 0, we will run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_top_titles_per_topic(topic_number=0, number_of_documents=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Topic Words in Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To display the original obituary texts that rank highly for a given topic, with the relevant topic words **bolded** for emphasis, we are going to make the function `display_bolded_topic_words_in_context()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "import re\n",
    "\n",
    "def display_bolded_topic_words_in_context(topic_number=3, number_of_documents=3, custom_words=None):\n",
    "\n",
    "    for probability, document in little_mallet_wrapper.get_top_docs(training_data, topic_distributions, topic_number, n=number_of_documents):\n",
    "        \n",
    "        print(f\"✨Topic {topic_number}✨\\n\\n{topics[topic_number]}\\n\")\n",
    "        \n",
    "        probability = f\"✨✨✨\\n\\n**{probability}**\"\n",
    "        obit_title = f\"**{training_data_obit_titles[document]}**\"\n",
    "        original_text = training_data_original_text[document]\n",
    "        topic_words = topics[topic_number]\n",
    "        topic_words = custom_words if custom_words != None else topic_words\n",
    "\n",
    "        for word in topic_words:\n",
    "            if word in original_text:\n",
    "                original_text = re.sub(f\"\\\\b{word}\\\\b\", f\"**{word}**\", original_text)\n",
    "\n",
    "        display(Markdown(probability)), display(Markdown(obit_title)), display(Markdown(original_text))\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Topic 3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To display the top 3 original obituaries with the highest probability of containing Topic 0 and with relevant topic words bolded, we will run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [],
   "source": [
    "display_bolded_topic_words_in_context(topic_number=3, number_of_documents=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Turn!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come up with a label for one of the topics. To do so, examine the top obituaries for that topic as well as the words in context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_top_titles_per_topic(topic_number=PICK-A-NUMBER, number_of_documents=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_bolded_topic_words_in_context(topic_number=PICK-A-NUMBER, number_of_documents=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Topic Label: *Your Label Here***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reflection**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why did you label your topic the way you did? What else could you or someone else have labeled it?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
