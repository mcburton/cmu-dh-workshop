{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-of-Speech Tagging for Chinese"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"admonition note\" name=\"html-admonition\" style=\"background: lightblue; padding: 10px\">\n",
    "<p class=\"title\">Note</p>\n",
    "This section, \"Working in Languages Beyond English,\" is co-authored with <a href=\"http://www.quinndombrowski.com/\">Quinn Dombrowski</a>, the Academic Technology Specialist at Stanford University and a leading voice in multilingual digital humanities. I'm grateful to Quinn for helping expand this textbook to serve languages beyond English. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we're going to learn about the textual analysis methods *part-of-speech tagging* and *keyword extraction* for Chinese-language texts. These methods will help us computationally parse sentences and better understand words in context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy and Natural Language Processing (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To computationally identify parts of speech, we're going to use the natural language processing library spaCy. For a more extensive introduction to NLP and spaCy, see the previous lesson.\n",
    "\n",
    "To parse sentences, spaCy relies on machine learning models that were trained on large amounts of labeled text data. If you've used the preprocessing or named entity recognition notebooks for this language, you can skip the steps for installing spaCy and downloading the language model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use spaCy, we first need to install the library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we're going to import `spacy` and `displacy`, a special spaCy module for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "pd.set_option(\"max_rows\", 400)\n",
    "pd.set_option(\"max_colwidth\", 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're also going to import the `Counter` module for counting nouns, verbs, adjectives, etc., and the `pandas` library for organizing and displaying data (we're also changing the pandas default max row and column width display setting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to download the Chinese-language model (`zh_core_web_md`), which will be processing and making predictions about our texts. The Chinese model was trained on the [OntoNotes](https://catalog.ldc.upenn.edu/LDC2013T19) annotated corpus. You can download the `zh_core_web_md` model by running the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting zh-core-web-md==3.1.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/zh_core_web_md-3.1.0/zh_core_web_md-3.1.0-py3-none-any.whl (78.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 78.8 MB 22.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting spacy-pkuseg<0.1.0,>=0.0.27\n",
      "  Downloading spacy_pkuseg-0.0.28-cp38-cp38-macosx_10_9_x86_64.whl (2.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.4 MB 7.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.2.0,>=3.1.0 in /Users/melwalsh/opt/anaconda3/lib/python3.8/site-packages (from zh-core-web-md==3.1.0) (3.1.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /Users/melwalsh/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->zh-core-web-md==3.1.0) (1.8.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/melwalsh/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->zh-core-web-md==3.1.0) (4.59.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /Users/melwalsh/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->zh-core-web-md==3.1.0) (0.7.4)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /Users/melwalsh/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->zh-core-web-md==3.1.0) (0.3.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/melwalsh/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->zh-core-web-md==3.1.0) (3.0.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /Users/melwalsh/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->zh-core-web-md==3.1.0) (0.8.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/melwalsh/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->zh-core-web-md==3.1.0) (1.0.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/melwalsh/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->zh-core-web-md==3.1.0) (2.25.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.7 in /Users/melwalsh/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->zh-core-web-md==3.1.0) (3.0.8)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /Users/melwalsh/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->zh-core-web-md==3.1.0) (2.4.1)\n",
      "Requirement already satisfied: jinja2 in /Users/melwalsh/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->zh-core-web-md==3.1.0) (2.11.3)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.8 in /Users/melwalsh/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->zh-core-web-md==3.1.0) (8.0.8)\n",
      "Requirement already satisfied: setuptools in /Users/melwalsh/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->zh-core-web-md==3.1.0) (52.0.0.post20210125)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/melwalsh/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->zh-core-web-md==3.1.0) (20.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/melwalsh/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->zh-core-web-md==3.1.0) (2.0.5)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/melwalsh/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->zh-core-web-md==3.1.0) (0.6.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/melwalsh/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->zh-core-web-md==3.1.0) (1.20.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.4 in /Users/melwalsh/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->zh-core-web-md==3.1.0) (2.0.6)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/melwalsh/opt/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.2.0,>=3.1.0->zh-core-web-md==3.1.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /Users/melwalsh/opt/anaconda3/lib/python3.8/site-packages (from pathy>=0.3.5->spacy<3.2.0,>=3.1.0->zh-core-web-md==3.1.0) (5.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/melwalsh/opt/anaconda3/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.2.0,>=3.1.0->zh-core-web-md==3.1.0) (3.7.4.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/melwalsh/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->zh-core-web-md==3.1.0) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/melwalsh/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->zh-core-web-md==3.1.0) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/melwalsh/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->zh-core-web-md==3.1.0) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/melwalsh/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->zh-core-web-md==3.1.0) (2020.12.5)\n",
      "Requirement already satisfied: cython>=0.25 in /Users/melwalsh/opt/anaconda3/lib/python3.8/site-packages (from spacy-pkuseg<0.1.0,>=0.0.27->zh-core-web-md==3.1.0) (0.29.23)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in /Users/melwalsh/opt/anaconda3/lib/python3.8/site-packages (from typer<0.4.0,>=0.3.0->spacy<3.2.0,>=3.1.0->zh-core-web-md==3.1.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/melwalsh/opt/anaconda3/lib/python3.8/site-packages (from jinja2->spacy<3.2.0,>=3.1.0->zh-core-web-md==3.1.0) (1.1.1)\n",
      "Installing collected packages: spacy-pkuseg, zh-core-web-md\n",
      "Successfully installed spacy-pkuseg-0.0.28 zh-core-web-md-3.1.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('zh_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download zh_core_web_md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: spaCy offers [models for other languages](https://spacy.io/usage/models#languages) including Chinese, German, French, Spanish, Portuguese, Russian, Italian, Dutch, Greek, Norwegian, and Lithuanian*.  \n",
    "\n",
    "*spaCy offers language and tokenization support for other language via external dependencies — such as [PyviKonlpy](https://github.com/konlpy/konlpy) for Korean.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is downloaded, we need to load it with `spacy.load()` and assign it to the variable `nlp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('zh_core_web_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Processed spaCy Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever we use spaCy, our first step will be to create a processed spaCy `document` with the loaded NLP model `nlp()`. Most of the heavy NLP lifting is done in this line of code. After processing, the `document` object will contain tons of juicy language data — named entities, sentence boundaries, parts of speech — and the rest of our work will be devoted to accessing this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '../texts/zh.txt'\n",
    "text = open(filepath, encoding='utf-8').read()\n",
    "document = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy Part-of-Speech Tagging\n",
    "The tags that spaCy uses for part-of-speech are based on work done by [Universal Dependencies](https://universaldependencies.org/), an effort to create a set of part-of-speech tags that work across many different languages. Texts from various languages are annotated using this common set of tags, and contributed to a common repository that can be used to train models like spaCy.\n",
    "\n",
    "The Universal Dependencies page has information about the annotated corpora available for each language; it's worth looking into the corpora that were annotated for your language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| POS   | Description               | Examples                                      |\n",
    "|:-----:|:-------------------------:|:---------------------------------------------:|\n",
    "| ADJ   | adjective                 | big, old, green, incomprehensible, first      |\n",
    "| ADP   | adposition                | in, to, during                                |\n",
    "| ADV   | adverb                    | very, tomorrow, down, where, there            |\n",
    "| AUX   | auxiliary                 | is, has (done), will (do), should (do)        |\n",
    "| CONJ  | conjunction               | and, or, but                                  |\n",
    "| CCONJ | coordinating conjunction  | and, or, but                                  |\n",
    "| DET   | determiner                | a, an, the                                    |\n",
    "| INTJ  | interjection              | psst, ouch, bravo, hello                      |\n",
    "| NOUN  | noun                      | girl, cat, tree, air, beauty                  |\n",
    "| NUM   | numeral                   | 1, 2017, one, seventy-seven, IV, MMXIV        |\n",
    "| PART  | particle                  | ’s, not,                                      |\n",
    "| PRON  | pronoun                   | I, you, he, she, myself, themselves, somebody |\n",
    "| PROPN | proper noun               | Mary, John, London, NATO, HBO                 |\n",
    "| PUNCT | punctuation               | ., (, ), ?                                    |\n",
    "| SCONJ | subordinating conjunction | if, while, that                               |\n",
    "| SYM   | symbol                    | $, %, §, ©, +, −, ×, ÷, =, :), 😝             |\n",
    "| VERB  | verb                      | run, runs, running, eat, ate, eating          |\n",
    "| X     | other                     | sfpksdpsxmsa                                  |\n",
    "| SPACE | space                     |                                               |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is a POS chart taken from [spaCy's website](https://spacy.io/api/annotation#named-entities), which shows the different parts of speech that spaCy can identify as well as their corresponding labels. To quickly see spaCy's POS tagging in action, we can use the [spaCy module `displacy`](https://spacy.io/usage/visualizers#ent) on our sample `document` with the `style=` parameter set to \"dep\" (short for dependency parsing):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Part-Of-Speech Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get part of speech tags for every word in a document, we have to iterate through all the tokens in the document and pull out the `.lemma_` attribute for each token, which gives us the un-inflected version of the word. We'll also pull out the  `.pos_` attribute for each token. We can get even finer-grained dependency information with the attribute `.dep_`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true,
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " PROPN nsubj\n",
      " PUNCT punct\n",
      " VERB ROOT\n",
      " PROPN nmod\n",
      " PROPN nmod\n",
      " ADJ amod\n",
      " NOUN dobj\n",
      " PUNCT punct\n",
      " SPACE nsubj\n",
      " PROPN dep\n",
      " NOUN advmod:loc\n",
      " PART case\n",
      " ADV advmod\n",
      " VERB amod\n",
      " PART mark\n",
      " NOUN dep\n",
      " PUNCT punct\n",
      " ADV advmod\n",
      " PRON dep\n",
      " PROPN dep\n",
      " ADJ amod\n",
      " NOUN dep\n",
      " PART discourse\n",
      " PUNCT ROOT\n",
      " ADP case\n",
      " NOUN nmod:prep\n",
      " VERB acl\n",
      " PUNCT punct\n",
      " VERB conj\n",
      " PART aux:asp\n",
      " ADJ amod\n",
      " NOUN dobj\n",
      " PUNCT punct\n",
      " ADV advmod\n",
      " VERB dep\n",
      " PART dep\n",
      " VERB conj\n",
      " VERB dep\n",
      " PART aux:asp\n",
      " NOUN compound:nn\n",
      " NOUN dobj\n",
      " PUNCT punct\n",
      " ADV neg\n",
      " VERB conj\n",
      " NOUN dobj\n",
      " PART mark\n",
      " PUNCT punct\n",
      " NOUN nsubj\n",
      " VERB ROOT\n",
      " PUNCT punct\n",
      " PUNCT punct\n",
      " VERB dep\n",
      " PUNCT punct\n",
      " ADV advmod\n",
      " VERB cop\n",
      " ADV dep\n",
      " VERB ccomp\n",
      " PART discourse\n",
      " PUNCT punct\n",
      " PUNCT punct\n",
      " PROPN nsubj\n",
      " VERB ROOT\n",
      " VERB advmod:rcomp\n",
      " VERB conj\n",
      " PUNCT punct\n",
      " ADV advmod\n",
      " VERB ROOT\n",
      " PART aux:asp\n",
      " PUNCT punct\n",
      " NOUN nmod:tmod\n",
      " VERB cop\n",
      " NOUN nmod:assmod\n",
      " PART case\n",
      " NOUN dobj\n",
      " PUNCT punct\n",
      " DET det\n",
      " NUM mark:clf\n",
      " NOUN dobj\n",
      " PUNCT punct\n",
      " NOUN conj\n",
      " PUNCT punct\n",
      " NOUN advmod:dvp\n",
      " PART mark\n",
      " VERB conj\n",
      " PUNCT punct\n",
      " ADV neg\n",
      " VERB conj\n",
      " NUM nmod:range\n",
      " NUM mark:clf\n",
      " PUNCT punct\n",
      " ADV advmod\n",
      " ADV neg\n",
      " VERB conj\n",
      " ADV advmod\n",
      " PUNCT punct\n",
      " ADV advmod\n",
      " X aux:ba\n",
      " NUM nummod\n",
      " NUM mark:clf\n",
      " VERB amod\n",
      " PROPN compound:vc\n",
      " PART mark\n",
      " PROPN dep\n",
      " PUNCT punct\n",
      " ADP case\n",
      " NOUN nmod:prep\n",
      " VERB conj\n",
      " PART aux:asp\n",
      " PUNCT punct\n",
      " ADV advmod\n",
      " VERB acl\n",
      " PART mark\n",
      " NOUN dep\n",
      " PUNCT punct\n",
      " ADV advmod\n",
      " ADV xcomp\n",
      " VERB conj\n",
      " ADV advmod\n",
      " PUNCT punct\n",
      " VERB conj\n",
      " PART aux:asp\n",
      " ADV advmod\n",
      " NOUN nsubj\n",
      " ADV advmod\n",
      " VERB conj\n",
      " PART discourse\n",
      " PUNCT punct\n",
      " NOUN nsubj\n",
      " ADV advmod\n",
      " VERB conj\n",
      " PART aux:asp\n",
      " PUNCT punct\n",
      " ADV advmod\n",
      " NOUN conj\n",
      " PUNCT punct\n",
      " NOUN conj\n",
      " PUNCT punct\n",
      " NOUN nmod:topic\n",
      " NUM dep\n",
      " NUM mark:clf\n",
      " PUNCT punct\n",
      " DET det\n",
      " NOUN compound:nn\n",
      " NOUN compound:nn\n",
      " NOUN compound:nn\n",
      " VERB conj\n",
      " PUNCT punct\n",
      " ADV dep\n",
      " PUNCT punct\n",
      " PRON nsubj\n",
      " ADV advmod\n",
      " VERB ROOT\n",
      " PUNCT punct\n",
      " VERB conj\n",
      " PART aux:asp\n",
      " NOUN nmod:assmod\n",
      " PART case\n",
      " NOUN dobj\n",
      " PUNCT punct\n",
      " ADV advmod\n",
      " VERB conj\n",
      " NUM nummod\n",
      " NUM mark:clf\n",
      " ADV advmod\n",
      " NOUN dobj\n",
      " PART discourse\n",
      " PUNCT punct\n",
      " SCONJ advmod\n",
      " NOUN nsubj\n",
      " VERB dep\n",
      " PUNCT punct\n",
      " ADV neg\n",
      " VERB conj\n",
      " NOUN compound:nn\n",
      " VERB dobj\n",
      " PUNCT punct\n",
      " NOUN nmod:assmod\n",
      " PART case\n",
      " NOUN compound:nn\n",
      " NOUN conj\n",
      " PUNCT punct\n",
      " NOUN compound:nn\n",
      " NOUN conj\n",
      " PUNCT punct\n",
      " ADV advmod\n",
      " ADV advmod\n",
      " VERB conj\n",
      " PART aux:asp\n",
      " PUNCT punct\n",
      " VERB dep\n",
      " PART aux:asp\n",
      " VERB acl\n",
      " PART mark\n",
      " NOUN dobj\n",
      " PUNCT punct\n",
      " ADP case\n",
      " NUM nummod\n",
      " NUM mark:clf\n",
      " VERB amod\n",
      " PART mark\n",
      " NOUN nmod:prep\n",
      " PUNCT punct\n",
      " VERB ROOT\n",
      " ADP case\n",
      " NOUN nmod:prep\n",
      " PUNCT punct\n",
      " NOUN nsubj\n",
      " ADV advmod\n",
      " ADV neg\n",
      " VERB aux:modal\n",
      " VERB conj\n",
      " PUNCT punct\n",
      " VERB dep\n",
      " PART aux:asp\n",
      " PRON dobj\n",
      " PUNCT punct\n",
      " SCONJ advmod\n",
      " VERB dep\n",
      " PART aux:asp\n",
      " NOUN dobj\n",
      " SCONJ advmod\n",
      " ADV neg\n",
      " VERB dep\n",
      " PUNCT punct\n",
      " ADV advmod\n",
      " ADV advmod\n",
      " VERB ROOT\n",
      " PUNCT punct\n",
      " ADV advmod\n",
      " VERB conj\n",
      " VERB ccomp\n",
      " VERB ccomp\n",
      " NOUN nsubj\n",
      " VERB ccomp\n",
      " PART aux:asp\n",
      " PUNCT punct\n",
      " VERB dep\n",
      " PART aux:asp\n",
      " ADV neg\n",
      " VERB dep\n",
      " PUNCT punct\n",
      " ADV advmod\n",
      " ADV advmod\n",
      " VERB ROOT\n",
      " PUNCT punct\n",
      " ADV advmod\n",
      " VERB ccomp\n",
      " PART aux:asp\n",
      " NOUN dobj\n",
      " PUNCT punct\n",
      " PUNCT punct\n",
      " ADV advmod\n",
      " PUNCT punct\n",
      " NOUN nsubj\n",
      " ADV neg\n",
      " VERB conj\n",
      " PUNCT punct\n",
      " PUNCT punct\n",
      " SCONJ advmod\n",
      " VERB dep\n",
      " NUM nummod\n",
      " NUM mark:clf\n",
      " NOUN ccomp\n",
      " PART discourse\n",
      " PUNCT punct\n",
      " CCONJ advmod\n",
      " VERB ROOT\n",
      " PART aux:asp\n",
      " NOUN dobj\n",
      " NUM nmod:range\n",
      " NUM mark:clf\n",
      " PUNCT punct\n",
      " VERB conj\n",
      " PART aux:asp\n",
      " NOUN dobj\n",
      " PUNCT punct\n",
      " ADV advmod\n",
      " VERB conj\n",
      " NOUN dobj\n",
      " ADP case\n",
      " NOUN nsubj\n",
      " VERB dep\n",
      " VERB advmod\n",
      " VERB conj\n",
      " PUNCT punct\n",
      " PUNCT punct\n",
      " ADV neg\n",
      " VERB ccomp\n",
      " PUNCT punct\n",
      " ADV neg\n",
      " VERB conj\n",
      " NOUN dobj\n",
      " PART discourse\n",
      " PUNCT punct\n",
      " PUNCT punct\n",
      " ADV advmod\n",
      " VERB dep\n",
      " PUNCT punct\n",
      " PRON nsubj\n",
      " VERB cop\n",
      " VERB dep\n",
      " ADV dobj\n",
      " ADV neg\n",
      " VERB dep\n",
      " PART discourse\n",
      " PUNCT ROOT\n",
      " ADV ROOT\n",
      " NUM nummod\n",
      " NUM mark:clf\n",
      " VERB amod\n",
      " PART mark\n",
      " NOUN dobj\n",
      " PUNCT punct\n",
      " NOUN nsubj\n",
      " VERB dep\n",
      " PART aux:asp\n",
      " PUNCT punct\n",
      " NOUN nsubj\n",
      " VERB xcomp\n",
      " VERB dep\n",
      " NUM nummod\n",
      " NOUN dobj\n",
      " PUNCT punct\n",
      " VERB conj\n",
      " NUM dobj\n",
      " PUNCT punct\n",
      " NOUN nsubj\n",
      " VERB dep\n",
      " PART aux:asp\n",
      " PUNCT punct\n",
      " NOUN nsubj\n",
      " ADV advmod\n",
      " VERB ROOT\n",
      " NUM nummod\n",
      " NUM mark:clf\n",
      " NOUN dobj\n",
      " PUNCT punct\n",
      " VERB conj\n",
      " NOUN nsubj\n",
      " VERB ccomp\n",
      " PART discourse\n",
      " PUNCT punct\n",
      " ADV advmod\n",
      " ADV advmod\n",
      " ADV neg\n",
      " VERB conj\n",
      " ADV punct\n",
      " NOUN nsubj\n",
      " VERB dep\n",
      " ADV advmod\n",
      " VERB conj\n",
      " NUM dep\n",
      " NUM mark:clf\n",
      " PUNCT punct\n",
      " ADV advmod\n",
      " VERB dep\n",
      " NOUN dobj\n",
      " PART nsubj\n",
      " ADV advmod\n",
      " ADV advmod\n",
      " VERB conj\n",
      " PUNCT punct\n",
      " ADJ amod\n",
      " NOUN nsubj\n",
      " ADV advmod\n",
      " VERB conj\n",
      " PART aux:asp\n",
      " PUNCT punct\n",
      " VERB dep\n",
      " NOUN dobj\n",
      " PUNCT punct\n",
      " NOUN nsubj\n",
      " ADV advmod\n",
      " VERB ROOT\n",
      " NOUN dobj\n",
      " PUNCT punct\n",
      " VERB ROOT\n",
      " NOUN dep\n",
      " VERB ccomp\n",
      " NOUN dobj\n",
      " PUNCT punct\n",
      " ADV advmod\n",
      " VERB conj\n",
      " DET det\n",
      " NOUN dobj\n",
      " VERB ccomp\n",
      " PART discourse\n",
      " PUNCT punct\n",
      " ADV advmod\n",
      " ADV advmod\n",
      " ADV dep\n",
      " NOUN dobj\n",
      " PART discourse\n",
      " PUNCT ROOT\n",
      " DET det\n",
      " NOUN nsubj\n",
      " PUNCT punct\n",
      " ADV advmod\n",
      " VERB ROOT\n",
      " PUNCT punct\n",
      " NOUN nsubj\n",
      " VERB cop\n",
      " NOUN ccomp\n",
      " PUNCT punct\n",
      " VERB conj\n",
      " VERB cop\n",
      " VERB ccomp\n",
      " NOUN dobj\n",
      " PART discourse\n",
      " PUNCT punct\n",
      " PUNCT punct\n",
      " ADV advmod\n",
      " ADV advmod\n",
      " X aux:ba\n",
      " NOUN dep\n",
      " VERB conj\n",
      " PROPN nmod:assmod\n",
      " PART case\n",
      " NOUN dobj\n",
      " ADV advmod\n",
      " VERB conj\n",
      " PUNCT punct\n",
      " ADV dep\n",
      " PUNCT punct\n",
      " ADV dep\n",
      " DET det\n",
      " NUM mark:clf\n",
      " NOUN dobj\n",
      " PART discourse\n",
      " PUNCT ROOT\n",
      " SPACE dep\n",
      " VERB ROOT\n",
      " PUNCT punct\n",
      " PRON nsubj\n",
      " VERB xcomp\n",
      " VERB dep\n",
      " NOUN compound:nn\n",
      " NOUN nsubj\n",
      " VERB cop\n",
      " VERB ccomp\n",
      " PART discourse\n",
      " PUNCT punct\n",
      " ADJ amod\n",
      " NOUN dep\n",
      " NOUN dobj\n",
      " VERB dep\n",
      " VERB dep\n",
      " PUNCT punct\n",
      " NOUN advmod\n",
      " DET det\n",
      " NOUN nsubj\n",
      " VERB ROOT\n",
      " DET det\n",
      " PUNCT punct\n",
      " NOUN amod\n",
      " PUNCT punct\n",
      " PUNCT punct\n",
      " PUNCT punct\n",
      " NOUN conj\n",
      " PUNCT punct\n",
      " PUNCT punct\n",
      " PUNCT punct\n",
      " NOUN compound:nn\n",
      " NOUN dep\n",
      " PUNCT punct\n",
      " DET det\n",
      " NOUN dobj\n",
      " PUNCT punct\n",
      " PRON nmod:poss\n",
      " NOUN nsubj\n",
      " SCONJ advmod\n",
      " VERB dep\n",
      " NOUN dobj\n",
      " PART mark\n",
      " PUNCT punct\n",
      " ADV advmod\n",
      " VERB aux:modal\n",
      " VERB conj\n",
      " NOUN dobj\n",
      " ADP case\n",
      " PRON nmod:prep\n",
      " VERB ccomp\n",
      " PUNCT punct\n",
      " PROPN nsubj\n",
      " VERB conj\n",
      " PART aux:asp\n",
      " DET det\n",
      " VERB acl\n",
      " PART mark\n",
      " NOUN dobj\n",
      " PUNCT punct\n",
      " PRON nsubj\n",
      " ADV advmod\n",
      " VERB dep\n",
      " NOUN dobj\n",
      " PART discourse\n",
      " PUNCT punct\n",
      " ADV advmod\n",
      " VERB aux:modal\n",
      " VERB conj\n",
      " VERB ccomp\n",
      " ADV advmod\n",
      " PUNCT punct\n",
      " ADV advmod\n",
      " PRON nsubj\n",
      " VERB conj\n",
      " PART aux:asp\n",
      " PRON nmod:assmod\n",
      " PART case\n",
      " NOUN dobj\n",
      " PUNCT punct\n",
      " PRON nsubj\n",
      " ADV neg\n",
      " VERB aux:modal\n",
      " ADV neg\n",
      " VERB dep\n",
      " PART discourse\n",
      " PART discourse\n",
      " PUNCT ROOT\n",
      " NOUN nsubj\n",
      " VERB ROOT\n",
      " PRON dep\n",
      " VERB ccomp\n",
      " NOUN dobj\n",
      " PUNCT punct\n",
      " VERB conj\n",
      " NOUN dobj\n",
      " PUNCT punct\n",
      " VERB conj\n",
      " PRON nmod:assmod\n",
      " PART case\n",
      " NOUN dobj\n",
      " PUNCT punct\n",
      " VERB conj\n",
      " PRON dobj\n",
      " VERB ccomp\n",
      " PUNCT punct\n",
      " PRON nsubj\n",
      " ADV advmod\n",
      " ADV neg\n",
      " VERB aux:modal\n",
      " ADP case\n",
      " PRON nmod:prep\n",
      " VERB dep\n",
      " PUNCT punct\n",
      " ADV advmod\n",
      " VERB conj\n",
      " PART aux:asp\n",
      " PART discourse\n",
      " PUNCT punct\n",
      " PRON nsubj\n",
      " ADV advmod\n",
      " PRON nmod:poss\n",
      " NOUN compound:nn\n",
      " PRON nsubj\n",
      " VERB ROOT\n",
      " NOUN dobj\n",
      " PUNCT punct\n",
      " ADV advmod\n",
      " NOUN nsubj\n",
      " ADV advmod\n",
      " VERB conj\n",
      " NOUN dobj\n",
      " VERB conj\n",
      " PART aux:asp\n",
      " PUNCT punct\n",
      " PRON nsubj\n",
      " ADV advmod\n",
      " VERB advmod\n",
      " VERB conj\n",
      " PUNCT punct\n",
      " NOUN dobj\n",
      " PUNCT punct\n",
      " NOUN nsubj\n",
      " VERB ROOT\n",
      " PRON nsubj\n",
      " VERB ccomp\n",
      " PUNCT punct\n",
      " PRON nsubj\n",
      " ADV advmod\n",
      " ADV neg\n",
      " VERB compound:vc\n",
      " VERB conj\n",
      " PRON nsubj\n",
      " VERB ccomp\n",
      " PUNCT punct\n",
      " SCONJ advmod\n",
      " NOUN advmod:loc\n",
      " PART case\n",
      " VERB dep\n",
      " PUNCT punct\n",
      " ADV advmod\n",
      " VERB conj\n",
      " NOUN dobj\n",
      " ADV advmod\n",
      " ADV neg\n",
      " VERB ccomp\n",
      " PART discourse\n",
      " PUNCT punct\n",
      " PRON nsubj\n",
      " ADV advmod\n",
      " VERB ROOT\n",
      " VERB ccomp\n",
      " PUNCT punct\n",
      " ADV advmod\n",
      " VERB nsubj\n",
      " ADV neg\n",
      " VERB conj\n",
      " PUNCT punct\n",
      " ADV advmod\n",
      " VERB conj\n",
      " NOUN nsubj\n",
      " VERB ccomp\n",
      " NOUN nsubj\n",
      " VERB ccomp\n",
      " PUNCT punct\n",
      " ADV advmod\n",
      " ADV advmod\n",
      " X aux:ba\n",
      " PRON dep\n",
      " VERB conj\n",
      " PART aux:asp\n",
      " PUNCT punct\n",
      " VERB conj\n",
      " NOUN dobj\n",
      " VERB ccomp\n",
      " VERB ccomp\n",
      " PUNCT punct\n",
      " NOUN nsubj\n",
      " VERB aux:modal\n",
      " ADV advmod\n",
      " VERB conj\n",
      " NOUN dobj\n",
      " PUNCT punct\n",
      " ADP case\n",
      " ADV neg\n",
      " VERB nmod:prep\n",
      " PRON nsubj\n",
      " VERB ccomp\n",
      " PUNCT punct\n",
      " VERB conj\n",
      " PUNCT punct\n",
      " PRON nsubj\n",
      " ADV advmod\n",
      " VERB cop\n",
      " VERB ROOT\n",
      " PART discourse\n",
      " PUNCT punct\n",
      " VERB conj\n",
      " NOUN dobj\n",
      " ADV neg\n",
      " VERB conj\n",
      " PART discourse\n",
      " PUNCT punct\n",
      " ADV advmod\n",
      " VERB ROOT\n",
      " PUNCT punct\n",
      " NOUN dep\n",
      " VERB dep\n",
      " NOUN nsubj\n",
      " VERB dobj\n",
      " PART discourse\n",
      " PART discourse\n",
      " PUNCT punct\n",
      " ADV advmod\n",
      " VERB cop\n",
      " VERB acl\n",
      " NOUN dobj\n",
      " PUNCT punct\n",
      " VERB conj\n",
      " NOUN dobj\n",
      " PUNCT punct\n",
      " ADV advmod\n",
      " VERB conj\n",
      " PART mark\n",
      " NOUN nsubj\n",
      " VERB dep\n",
      " PART aux:asp\n",
      " NOUN dobj\n",
      " PUNCT punct\n",
      " PRON nsubj\n",
      " VERB conj\n",
      " PRON nmod:assmod\n",
      " PART case\n",
      " NOUN dobj\n",
      " PART discourse\n",
      " PUNCT ROOT\n",
      " ADV advmod\n",
      " VERB ROOT\n",
      " PART aux:asp\n",
      " PRON nmod:assmod\n",
      " PART case\n",
      " NOUN dobj\n",
      " PUNCT punct\n",
      " ADV advmod\n",
      " ADV neg\n",
      " VERB dep\n",
      " NOUN dobj\n",
      " PART discourse\n",
      " PUNCT punct\n",
      " VERB ROOT\n",
      " PUNCT punct\n",
      " ADV advmod\n",
      " ADV advmod\n",
      " VERB conj\n",
      " PART dep\n",
      " NOUN dobj\n",
      " PART discourse\n",
      " PUNCT punct\n",
      " DET det\n",
      " NOUN nsubj\n",
      " PUNCT punct\n",
      " VERB dep\n",
      " VERB advmod:rcomp\n",
      " PUNCT punct\n",
      " PRON nsubj\n",
      " ADV advmod\n",
      " VERB ROOT\n",
      " VERB ccomp\n",
      " PUNCT punct\n",
      " ADV advmod\n",
      " VERB dep\n",
      " ADV advmod\n",
      " NUM dep\n",
      " NOUN dobj\n",
      " PUNCT punct\n",
      " ADV advmod\n",
      " ADV advmod\n",
      " ADP case\n",
      " PRON nmod:prep\n",
      " VERB conj\n",
      " PUNCT punct\n",
      " SPACE dep\n",
      " ADV advmod\n",
      " ADV advmod\n",
      " PUNCT punct\n",
      " PRON nsubj\n",
      " VERB ROOT\n",
      " PRON compound:nn\n",
      " NOUN dobj\n",
      " PUNCT punct\n",
      " X aux:ba\n",
      " NOUN det\n",
      " NOUN dep\n",
      " PUNCT punct\n",
      " ADV advmod\n",
      " PROPN dep\n",
      " PUNCT punct\n",
      " X aux:ba\n",
      " NOUN amod\n",
      " NOUN dep\n",
      " PUNCT punct\n",
      " ADV advmod\n",
      " VERB conj\n",
      " PUNCT punct\n",
      " ADV advmod\n",
      " ADP case\n",
      " VERB nmod:prep\n",
      " PART aux:asp\n",
      " PUNCT punct\n",
      " NOUN nmod:tmod\n",
      " ADV advmod\n",
      " VERB advmod\n",
      " VERB conj\n",
      " PART discourse\n",
      " PUNCT punct\n",
      " VERB dep\n",
      " PART mark\n",
      " PART dep\n",
      " PUNCT punct\n",
      " ADV advmod\n",
      " VERB ROOT\n",
      " PUNCT punct\n",
      " ADJ dep\n",
      " ADV advmod\n",
      " VERB ccomp\n",
      " PUNCT punct\n",
      " PUNCT punct\n",
      " VERB conj\n",
      " NOUN nsubj\n",
      " ADV advmod\n",
      " VERB xcomp\n",
      " NOUN dobj\n",
      " PUNCT punct\n",
      " ADV advmod\n",
      " VERB conj\n",
      " PRON dobj\n",
      " ADP punct\n",
      " NOUN nsubj\n",
      " VERB conj\n",
      " PUNCT punct\n",
      " VERB xcomp\n",
      " VERB compound:vc\n",
      " VERB conj\n",
      " PUNCT punct\n",
      " ADV advmod\n",
      " VERB conj\n",
      " PRON dobj\n",
      " PUNCT punct\n",
      " NOUN nsubj\n",
      " VERB dep\n",
      " NOUN dobj\n",
      " PART mark\n",
      " PUNCT punct\n",
      " ADV advmod\n",
      " ADV advmod\n",
      " VERB ROOT\n",
      " PART aux:asp\n",
      " NOUN nmod:assmod\n",
      " PART case\n",
      " NOUN dobj\n",
      " PUNCT punct\n",
      " VERB conj\n",
      " PRON dobj\n",
      " VERB ccomp\n",
      " VERB compound:vc\n",
      " PUNCT punct\n",
      " NOUN nsubj\n",
      " VERB conj\n",
      " PUNCT punct\n",
      " NOUN nsubj\n",
      " ADV neg\n",
      " ADV advmod\n",
      " VERB conj\n",
      " PART aux:asp\n",
      " NOUN dobj\n",
      " PUNCT punct\n",
      " VERB xcomp\n",
      " VERB conj\n",
      " PRON dobj\n",
      " NOUN dobj\n",
      " PUNCT punct\n",
      " NOUN nsubj\n",
      " ADV advmod\n",
      " VERB cop\n",
      " VERB conj\n",
      " PUNCT punct\n",
      " ADV advmod\n",
      " ADV advmod\n",
      " ADP case\n",
      " PRON nmod:prep\n",
      " VERB conj\n",
      " PUNCT punct\n",
      " NOUN compound:nn\n",
      " NOUN dep\n",
      " PART case\n",
      " PART dep\n",
      " PUNCT punct\n",
      " SCONJ advmod\n",
      " VERB aux:modal\n",
      " NOUN dep\n",
      " ADV advmod\n",
      " VERB dep\n",
      " ADV advmod\n",
      " VERB ROOT\n",
      " PUNCT punct\n",
      " ADP case\n",
      " NOUN nmod:prep\n",
      " ADV advmod\n",
      " VERB xcomp\n",
      " ADV advmod\n",
      " VERB conj\n",
      " NOUN dobj\n",
      " PUNCT punct\n",
      " VERB conj\n",
      " PUNCT punct\n",
      " ADJ advmod\n",
      " VERB dep\n",
      " PART mark\n",
      " PART dep\n",
      " PUNCT punct\n",
      " VERB xcomp\n",
      " VERB ROOT\n",
      " NOUN compound:nn\n",
      " NOUN conj\n",
      " PUNCT punct\n",
      " NOUN dobj\n",
      " PUNCT punct\n",
      " VERB conj\n",
      " DET det\n",
      " ADP case\n",
      " NOUN nmod:prep\n",
      " VERB amod\n",
      " PART mark\n",
      " NOUN dobj\n",
      " PUNCT punct\n",
      " VERB dep\n",
      " PART mark\n",
      " PART dep\n",
      " PUNCT punct\n",
      " VERB xcomp\n",
      " VERB ROOT\n",
      " PART aux:asp\n",
      " NOUN compound:nn\n",
      " NOUN dobj\n",
      " PUNCT punct\n",
      " ADV advmod\n",
      " VERB conj\n",
      " VERB dobj\n",
      " PUNCT punct\n",
      " ADV nsubj\n",
      " VERB cop\n",
      " PRON nmod:assmod\n",
      " PART case\n",
      " NOUN ROOT\n",
      " PART discourse\n",
      " PUNCT punct\n",
      " ADV dep\n",
      " VERB dep\n",
      " NOUN nsubj\n",
      " NOUN ccomp\n",
      " PART mark\n",
      " PART discourse\n",
      " PUNCT punct\n",
      " NOUN dep\n",
      " PRON nsubj\n",
      " ADV advmod\n",
      " ADV neg\n",
      " VERB ROOT\n",
      " PUNCT punct\n",
      " PRON nsubj\n",
      " ADV advmod\n",
      " VERB conj\n",
      " VERB ccomp\n",
      " PRON dobj\n",
      " PART discourse\n",
      " PUNCT punct\n",
      " PRON dep\n",
      " PRON nsubj\n",
      " ADV advmod\n",
      " VERB ROOT\n",
      " PUNCT punct\n",
      " VERB acl\n",
      " NOUN dobj\n",
      " PART mark\n",
      " NOUN nmod:prep\n",
      " PUNCT punct\n",
      " ADV advmod\n",
      " ADV advmod\n",
      " VERB dep\n",
      " PART discourse\n",
      " PUNCT punct\n",
      " DET det\n",
      " NUM nsubj\n",
      " NUM dep\n",
      " ADV neg\n",
      " VERB aux:modal\n",
      " VERB ROOT\n",
      " PRON nmod:assmod\n",
      " PART case\n",
      " NOUN nsubj\n",
      " ADV advmod\n",
      " VERB ccomp\n",
      " PART discourse\n",
      " PUNCT punct\n"
     ]
    }
   ],
   "source": [
    "for token in document:\n",
    "    print(token.lemma_, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practicing with the example text\n",
    "When working with languages that have inflection, we typically use `token.lemma_` instead of `token.text` like you'll find in the English examples. This is important when we're counting, so that differently-inflected forms of a word (e.g. masculine vs. feminine or singular vs. plural) aren't counted as if they were different words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"../texts/zh.txt\"\n",
    "document = nlp(open(filepath, encoding=\"utf-8\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Adjectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| POS   | Description               | Examples                                      |\n",
    "|:-----:|:-------------------------:|:---------------------------------------------:|\n",
    "| ADJ   | adjective                 | big, old, green, incomprehensible, first      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract and count the adjectives in the example text, we will follow the same model as above, except we'll add an `if` statement that will pull out words only if their POS label matches \"ADJ.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"admonition pythonreview\" name=\"html-admonition\" style=\"background: lightgreen; padding: 10px\">\n",
    "<p class=\"title\">Python Review</p>\n",
    "\n",
    "While we demonstrate how to extract parts of speech in the sections below, we're also going to reinforce some integral Python skills. Notice how we use `for` loops and `if` statements to `.append()` specific words to a list. Then we count the words in the list and make a pandas dataframe from the list.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we make a list of the adjectives identified in the example text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "adjs = []\n",
    "for token in document:\n",
    "    if token.pos_ == 'ADJ':\n",
    "        adjs.append(token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true,
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '', '', '', '', '', '']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we count the unique adjectives in this list with the `Counter()` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "adjs_tally = Counter(adjs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true,
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 7)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjs_tally.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we make a dataframe from this list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true,
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adj</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  adj  count\n",
       "0          7"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(adjs_tally.most_common(), columns=['adj', 'count'])\n",
    "df[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| POS   | Description               | Examples                                      |\n",
    "|:-----:|:-------------------------:|:---------------------------------------------:|\n",
    "| NOUN  | noun                      | girl, cat, tree, air, beauty                  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract and count nouns, we can follow the same model as above, except we will change our `if` statement to check for POS labels that match \"NOUN\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true,
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>noun</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  noun  count\n",
       "0         159"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nouns = []\n",
    "for token in document:\n",
    "    if token.pos_ == 'NOUN':\n",
    "        nouns.append(token.lemma_)\n",
    "\n",
    "nouns_tally = Counter(nouns)\n",
    "\n",
    "df = pd.DataFrame(nouns_tally.most_common(), columns=['noun', 'count'])\n",
    "df[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Verbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| POS   | Description               | Examples                                      |\n",
    "|:-----:|:-------------------------:|:---------------------------------------------:|\n",
    "| VERB  | verb                      | run, runs, running, eat, ate, eating          |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract and count works of art, we can follow a similar-ish model to the examples above. This time, however, we're going to make our code even more economical and efficient (while still changing our `if` statement to match the POS label \"VERB\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"admonition pythonreview\" name=\"html-admonition\" style=\"background: lightgreen; padding: 10px\">\n",
    "<p class=\"title\">Python Review</p>\n",
    "\n",
    "We can use a [*list comprehension*](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Python/More-Lists-Loops.html#List-Comprehensions) to get our list of verbs in a single line of code! Closely examine the first line of code below:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true,
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>verb</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>232</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  verb  count\n",
       "0         232"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verbs = [token.lemma_ for token in document if token.pos_ == 'VERB']\n",
    "\n",
    "verbs_tally = Counter(verbs)\n",
    "\n",
    "df = pd.DataFrame(verbs_tally.most_common(), columns=['verb', 'count'])\n",
    "df[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Sentences with Keyword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy can also identify sentences in a document. To access sentences, we can iterate through `document.sents` and pull out the `.text` of each sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use spaCy's sentence-parsing capabilities to extract sentences that contain particular keywords, such as in the function below. Note that the function assumes that the keyword provided will be exactly the same as it appears in the text.\n",
    "\n",
    "With the function `find_sentences_with_keyword()`, we will iterate through `document.sents` and pull out any sentence that contains a particular \"keyword.\" Then we will display these sentence with the keywords bolded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sentences_with_keyword(keyword, document):\n",
    "    \n",
    "    #Iterate through all the sentences in the document and pull out the text of each sentence\n",
    "    for sentence in document.sents:\n",
    "        sentence = sentence.text\n",
    "        \n",
    "        #Check to see if the keyword is in the sentence (and ignore capitalization by making both lowercase)\n",
    "        if keyword.lower() in sentence.lower():\n",
    "            \n",
    "            #Use the regex library to replace linebreaks and to make the keyword bolded, again ignoring capitalization\n",
    "            sentence = re.sub('\\n', ' ', sentence)\n",
    "            sentence = re.sub(f\"{keyword}\", f\"**{keyword}**\", sentence, flags=re.IGNORECASE)\n",
    "            \n",
    "            display(Markdown(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true,
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "这还不说，到了择亲的时光，只凭着两个不要脸媒人的话，只要男家有钱有势，不问身家清白，**男人**的性情好坏、学问高低，就不知不觉应了。"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "到了那边，要是遇着**男人**虽不怎么样，却还安分，这就算前生有福今生受了。"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "要是说一二句抱怨的话，或是劝了**男人**几句，反了腔，就打骂俱下;别人听见还要说：“不贤惠，不晓得妇道呢!”"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "女子死了，**男人**只带几根蓝辫线，有嫌难看的，连带也不带;人死还没三天，就出去偷鸡摸狗;七还未尽，新娘子早已进门了。"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "自己又看看无功受禄，恐怕行不长久，一听见男子喜欢脚小，就急急忙忙把它缠了，使**男人**看见喜欢，庶可以藉此吃白饭。"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "自然是有学问、有见识、出力作事的**男人**得了权利，我们作他的奴隶了。"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "诸位晓得国是要亡的了，**男人**自己也不保，我们还想靠他么?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "find_sentences_with_keyword(keyword=\"男人\", document=document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Keyword in Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also find out about a keyword's more immediate context — its neighboring words to the left and right — and we can fine-tune our search with POS tagging.\n",
    "\n",
    "To do so, we will first create a list of what's called *ngrams*. \"Ngrams\" are any sequence of *n* tokens in a text. They're an important concept in computational linguistics and NLP. (Have you ever played with [Google's *Ngram* Viewer](https://books.google.com/ngrams)?)\n",
    "\n",
    "Below we're going to make a list of *bigrams*, that is, all the two-word combinations from the sample text. We're going to use these bigrams to find the neighboring words that appear alongside particular keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a list of tokens and POS labels from document if the token is a word \n",
    "tokens_and_labels = [(token.text, token.pos_) for token in document if token.is_alpha]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a function to get all two-word combinations\n",
    "def get_bigrams(word_list, number_consecutive_words=2):\n",
    "    \n",
    "    ngrams = []\n",
    "    adj_length_of_word_list = len(word_list) - (number_consecutive_words - 1)\n",
    "    \n",
    "    #Loop through numbers from 0 to the (slightly adjusted) length of your word list\n",
    "    for word_index in range(adj_length_of_word_list):\n",
    "        \n",
    "        #Index the list at each number, grabbing the word at that number index as well as N number of words after it\n",
    "        ngram = word_list[word_index : word_index + number_consecutive_words]\n",
    "        \n",
    "        #Append this word combo to the master list \"ngrams\"\n",
    "        ngrams.append(ngram)\n",
    "        \n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = get_bigrams(tokens_and_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a peek at the bigrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true,
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('同胞', 'NOUN'), ('世界', 'NOUN')],\n",
       " [('世界', 'NOUN'), ('上', 'PART')],\n",
       " [('上', 'PART'), ('最', 'ADV')],\n",
       " [('最', 'ADV'), ('不平', 'VERB')],\n",
       " [('不平', 'VERB'), ('的', 'PART')],\n",
       " [('的', 'PART'), ('事', 'NOUN')],\n",
       " [('事', 'NOUN'), ('就是', 'ADV')],\n",
       " [('就是', 'ADV'), ('我们', 'PRON')],\n",
       " [('我们', 'PRON'), ('二万万', 'PROPN')],\n",
       " [('二万万', 'PROPN'), ('女', 'ADJ')],\n",
       " [('女', 'ADJ'), ('同胞', 'NOUN')],\n",
       " [('同胞', 'NOUN'), ('了', 'PART')],\n",
       " [('了', 'PART'), ('从', 'ADP')],\n",
       " [('从', 'ADP'), ('小生', 'NOUN')],\n",
       " [('小生', 'NOUN'), ('下来', 'VERB')]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams[5:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our list of bigrams, we're going to make a function `get_neighbor_words()`. This function will return the most frequent words that appear next to a particular keyword. The function can also be fine-tuned to return neighbor words that match a certain part of speech by changing the `pos_label` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbor_words(keyword, bigrams, pos_label = None):\n",
    "    \n",
    "    neighbor_words = []\n",
    "    keyword = keyword.lower()\n",
    "    \n",
    "    for bigram in bigrams:\n",
    "        \n",
    "        #Extract just the lowercased words (not the labels) for each bigram\n",
    "        words = [word.lower() for word, label in bigram]        \n",
    "        \n",
    "        #Check to see if keyword is in the bigram\n",
    "        if keyword in words:\n",
    "            \n",
    "            for word, label in bigram:\n",
    "                \n",
    "                #Now focus on the neighbor word, not the keyword\n",
    "                if word.lower() != keyword:\n",
    "                    #If the neighbor word matches the right pos_label, append it to the master list\n",
    "                    if label == pos_label or pos_label == None:\n",
    "                        neighbor_words.append(word.lower())\n",
    "    \n",
    "    return Counter(neighbor_words).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true,
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('了', 3),\n",
       " ('的', 2),\n",
       " ('清白', 1),\n",
       " ('着', 1),\n",
       " ('虽', 1),\n",
       " ('几', 1),\n",
       " ('只', 1),\n",
       " ('使', 1),\n",
       " ('看见', 1),\n",
       " ('得', 1),\n",
       " ('自己', 1)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_neighbor_words(\"男人\", bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('清白', 1), ('使', 1), ('看见', 1), ('得', 1)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_neighbor_words(\"男人\", bigrams, pos_label='VERB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Turn!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try out `find_sentences_with_keyword()` and `get_neighbor_words` with your own keywords of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_sentences_with_keyword(keyword=\"YOUR KEY WORD\", document=document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_neighbor_words(keyword=\"YOUR KEY WORD\", bigrams, pos_label=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
